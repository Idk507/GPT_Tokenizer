{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fda2d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import torch  \n",
    "from collections import defaultdict,Counter \n",
    "from typing import List,Dict,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2633e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab: Dict[int, bytes] = {}\n",
    "        self.merges: Dict[Tuple[int, int], int] = {}\n",
    "        self.inverse_vocab: Dict[bytes, int] = {}\n",
    "        self.pattern = re.compile(\n",
    "            r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        )\n",
    "        self.special_tokens: Dict[str, int] = {}\n",
    "        self.special_tokens_inverse: Dict[int, str] = {}\n",
    "    def add_special_token(self, token: str) -> int:\n",
    "        \"\"\"Add a special token to the vocabulary\"\"\"\n",
    "        if token in self.special_tokens:\n",
    "            return self.special_tokens[token]\n",
    "        \n",
    "        token_id = len(self.vocab) + len(self.special_tokens)\n",
    "        self.special_tokens[token] = token_id\n",
    "        self.special_tokens_inverse[token_id] = token\n",
    "        return token_id\n",
    "\n",
    "    def train(self, text: str, vocab_size: int, verbose: bool = False):\n",
    "        \"\"\"Train the tokenizer on the given text\"\"\"\n",
    "        assert vocab_size >= 256, \"Vocabulary size must be at least 256\"\n",
    "        \n",
    "        # Initialize vocabulary with byte-level tokens\n",
    "        self.vocab = {i: bytes([i]) for i in range(256)}\n",
    "        self.merges = {}\n",
    "        \n",
    "        # Pre-tokenize the text\n",
    "        words = self._pre_tokenize(text)\n",
    "        \n",
    "        # Get initial frequencies\n",
    "        frequencies = self._get_stats(words)\n",
    "        \n",
    "        # Perform merges until we reach desired vocab size\n",
    "        while len(self.vocab) + len(self.special_tokens) < vocab_size:\n",
    "            if not frequencies:\n",
    "                break\n",
    "                \n",
    "            # Find most frequent pair\n",
    "            best_pair = max(frequencies, key=frequencies.get)\n",
    "            best_freq = frequencies[best_pair]\n",
    "            \n",
    "            if best_freq < 2:\n",
    "                break\n",
    "                \n",
    "            # Create new token\n",
    "            new_id = len(self.vocab)\n",
    "            merged = self.vocab[best_pair[0]] + self.vocab[best_pair[1]]\n",
    "            self.vocab[new_id] = merged\n",
    "            \n",
    "            # Store the merge\n",
    "            self.merges[best_pair] = new_id\n",
    "            \n",
    "            # Update all words with this pair\n",
    "            words = self._merge_words(words, best_pair, new_id)\n",
    "            \n",
    "            # Recompute frequencies\n",
    "            frequencies = self._get_stats(words)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Merged {best_pair} into {new_id} (freq: {best_freq})\")\n",
    "        \n",
    "        # Build inverse vocabulary for faster encoding\n",
    "        self.inverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "    def _pre_tokenize(self, text: str) -> List[List[int]]:\n",
    "        \"\"\"Split text into words and convert to byte IDs\"\"\"\n",
    "        words = []\n",
    "        for token in re.findall(self.pattern, text):\n",
    "            # Convert to bytes then to list of byte IDs\n",
    "            byte_ids = list(token.encode('utf-8'))\n",
    "            words.append(byte_ids)\n",
    "        return words\n",
    "    \n",
    "    def _get_stats(self, words: List[List[int]]) -> Dict[Tuple[int, int], int]:\n",
    "        \"\"\"Count frequency of adjacent pairs\"\"\"\n",
    "        frequencies = Counter()\n",
    "        for word in words:\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i+1])\n",
    "                frequencies[pair] += 1\n",
    "        return frequencies\n",
    "    \n",
    "    def _merge_words(self, words: List[List[int]], pair: Tuple[int, int], new_id: int) -> List[List[int]]:\n",
    "        \"\"\"Merge the given pair in all words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:\n",
    "                    new_word.append(new_id)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Encode text into tensor of token IDs\"\"\"\n",
    "        # Handle special tokens first\n",
    "        for token, token_id in sorted(self.special_tokens.items(), key=lambda x: -len(x[0])):\n",
    "            if token in text:\n",
    "                # This is simplified - real implementation would need to handle splits\n",
    "                pass\n",
    "        \n",
    "        # Pre-tokenize\n",
    "        words = self._pre_tokenize(text)\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Convert to list of byte IDs\n",
    "            current_word = word.copy()\n",
    "            \n",
    "            # Keep merging until no more merges possible\n",
    "            while True:\n",
    "                # Find all possible merges\n",
    "                merges = []\n",
    "                for i in range(len(current_word) - 1):\n",
    "                    pair = (current_word[i], current_word[i+1])\n",
    "                    if pair in self.merges:\n",
    "                        merges.append((i, pair))\n",
    "                \n",
    "                if not merges:\n",
    "                    break\n",
    "                \n",
    "                # Apply merges greedily (leftmost first)\n",
    "                merges.sort()\n",
    "                i, pair = merges[0]\n",
    "                merged_id = self.merges[pair]\n",
    "                current_word = current_word[:i] + [merged_id] + current_word[i+2:]\n",
    "            \n",
    "            tokens.extend(current_word)\n",
    "        \n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "    \n",
    "    def decode(self, token_ids: torch.Tensor) -> str:\n",
    "        \"\"\"Decode tensor of token IDs back to text\"\"\"\n",
    "        if token_ids.dim() == 0:\n",
    "            token_ids = token_ids.unsqueeze(0)\n",
    "            \n",
    "        bytes_list = []\n",
    "        for token_id in token_ids.tolist():\n",
    "            if token_id in self.special_tokens_inverse:\n",
    "                # Handle special tokens\n",
    "                bytes_list.append(self.special_tokens_inverse[token_id].encode('utf-8'))\n",
    "            else:\n",
    "                bytes_list.append(self.vocab[token_id])\n",
    "        \n",
    "        # Concatenate all bytes and decode\n",
    "        return b''.join(bytes_list).decode('utf-8', errors='replace')\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save tokenizer to file\"\"\"\n",
    "        state = {\n",
    "            'vocab': self.vocab,\n",
    "            'merges': self.merges,\n",
    "            'special_tokens': self.special_tokens,\n",
    "            'pattern': self.pattern.pattern\n",
    "        }\n",
    "        torch.save(state, filepath)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath: str) -> 'GPTTokenizer':\n",
    "        \"\"\"Load tokenizer from file\"\"\"\n",
    "        state = torch.load(filepath)\n",
    "        tokenizer = cls()\n",
    "        tokenizer.vocab = state['vocab']\n",
    "        tokenizer.merges = state['merges']\n",
    "        tokenizer.special_tokens = state['special_tokens']\n",
    "        tokenizer.special_tokens_inverse = {v: k for k, v in state['special_tokens'].items()}\n",
    "        tokenizer.pattern = re.compile(state['pattern'])\n",
    "        tokenizer.inverse_vocab = {v: k for k, v in state['vocab'].items()}\n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0696726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The GPT tokenizer is a crucial component of language models.\n",
    "It converts text into tokens that the model can process.\n",
    "This implementation shows how a BPE tokenizer works with PyTorch.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ffd7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPTTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99548f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_token(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd5c7c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged (32, 116) into 256 (freq: 6)\n",
      "Merged (101, 110) into 257 (freq: 5)\n",
      "Merged (32, 99) into 258 (freq: 4)\n",
      "Merged (256, 111) into 259 (freq: 3)\n",
      "Merged (259, 107) into 260 (freq: 3)\n",
      "Merged (260, 257) into 261 (freq: 3)\n",
      "Merged (101, 114) into 262 (freq: 3)\n",
      "Merged (32, 105) into 263 (freq: 3)\n",
      "Merged (111, 110) into 264 (freq: 3)\n",
      "Merged (84, 104) into 265 (freq: 2)\n",
      "Merged (261, 105) into 266 (freq: 2)\n",
      "Merged (266, 122) into 267 (freq: 2)\n",
      "Merged (267, 262) into 268 (freq: 2)\n",
      "Merged (32, 97) into 269 (freq: 2)\n",
      "Merged (109, 112) into 270 (freq: 2)\n",
      "Merged (257, 116) into 271 (freq: 2)\n",
      "Merged (97, 110) into 272 (freq: 2)\n",
      "Merged (32, 109) into 273 (freq: 2)\n",
      "Merged (273, 111) into 274 (freq: 2)\n",
      "Merged (274, 100) into 275 (freq: 2)\n",
      "Merged (275, 101) into 276 (freq: 2)\n",
      "Merged (276, 108) into 277 (freq: 2)\n",
      "Merged (46, 10) into 278 (freq: 2)\n",
      "Merged (256, 104) into 279 (freq: 2)\n",
      "Merged (97, 116) into 280 (freq: 2)\n",
      "Merged (104, 111) into 281 (freq: 2)\n",
      "Merged (281, 119) into 282 (freq: 2)\n",
      "Merged (32, 119) into 283 (freq: 2)\n",
      "Merged (111, 114) into 284 (freq: 2)\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(text, vocab_size=300, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "042d81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"Dhanushkumar.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92d63dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f88c1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 68, 104, 272, 117, 115, 104, 107, 117, 109,  97, 114,  46])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46190c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bf01e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dhanushkumar.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fbb440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Dhanushkumar.\n",
      "Encoded tokens: tensor([ 68, 104, 272, 117, 115, 104, 107, 117, 109,  97, 114,  46])\n",
      "Decoded text: Dhanushkumar.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOriginal text:\", test_text)\n",
    "print(\"Encoded tokens:\", encoded)\n",
    "print(\"Decoded text:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9f34d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"gpt_tokenizer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc4a28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tokenizer = GPTTokenizer.load(\"gpt_tokenizer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a1f4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert loaded_tokenizer.decode(loaded_tokenizer.encode(test_text)) == test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a35e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
